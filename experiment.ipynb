{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import Callable, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    \"\"\"\n",
    "    Experiment class to run optimization experiments with second order methods,\n",
    "    on a given function g, with computable gradient and hessian.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        g: Callable,\n",
    "        g_grad: Callable,\n",
    "        g_grad_and_hessian: Callable,\n",
    "        true_theta: np.ndarray = None,\n",
    "        true_hessian: np.ndarray = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the experiment\n",
    "        \"\"\"\n",
    "        self.true_theta = true_theta\n",
    "        self.true_hessian = true_hessian\n",
    "        self.g = g\n",
    "        self.g_grad = g_grad\n",
    "        self.g_grad_and_hessian = g_grad_and_hessian\n",
    "        if true_theta is not None:\n",
    "            self.true_theta = true_theta\n",
    "        if true_hessian is not None:\n",
    "            self.true_hessian = true_hessian\n",
    "\n",
    "    def set_e(self, e: float):\n",
    "        \"\"\"\n",
    "        Set the noise level for generating random initial theta\n",
    "        \"\"\"\n",
    "        self.e = e\n",
    "\n",
    "    def set_theta(self, theta: np.ndarray):\n",
    "        \"\"\"\n",
    "        Set the initial theta\n",
    "        \"\"\"\n",
    "        self.theta = theta\n",
    "\n",
    "    def generate_initial_theta(self):\n",
    "        \"\"\"\n",
    "        Generate a random initial theta\n",
    "        \"\"\"\n",
    "        if self.e is None:\n",
    "            raise ValueError(\"e is not set for generating random theta\")\n",
    "        self.theta = (\n",
    "            self.true_theta + np.random.randn(self.true_theta.shape[0]) * self.e\n",
    "        )\n",
    "\n",
    "    def set_optimizer(self, optimizer: Callable):\n",
    "        \"\"\"\n",
    "        Set the optimizer\n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer\n",
    "        self.optimizer.g = self.g\n",
    "        self.optimizer.g_grad = self.g_grad\n",
    "        self.optimizer.g_grad_and_hessian = self.g_grad_and_hessian\n",
    "        self.optimizer.reset_lr()\n",
    "\n",
    "    def set_dataset(self, dataset: List[Tuple[np.ndarray, np.ndarray]]):\n",
    "        \"\"\"\n",
    "        Set the dataset\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def run(self, plot: bool = False) -> Tuple[List[float], List[float]]:\n",
    "        \"\"\"\n",
    "        Run the experiment for a given theta, optimizer and dataset\n",
    "        \"\"\"\n",
    "        if self.theta is None:\n",
    "            raise ValueError(\"Theta is not set\")\n",
    "        if self.optimizer is None:\n",
    "            raise ValueError(\"Optimizer is not set\")\n",
    "        if self.dataset is None:\n",
    "            raise ValueError(\"Dataset is not set\")\n",
    "\n",
    "        self.hessian = np.eye(self.theta.shape[0])  # Reset hessian to identity\n",
    "        if self.true_theta is not None:\n",
    "            theta_error = [\n",
    "                np.dot(self.theta - self.true_theta, self.theta - self.true_theta)\n",
    "            ]\n",
    "        if self.true_hessian is not None:\n",
    "            hessian_error = [\n",
    "                np.linalg.norm(self.hessian - self.true_hessian, ord=\"fro\")\n",
    "            ]\n",
    "        self.optimizer.reset_lr()\n",
    "\n",
    "        for X, Y in tqdm(self.dataset, desc=\"Optimizing\", leave=False):\n",
    "            # One step of optimization\n",
    "            self.theta, self.hessian = self.optimizer.step(self.theta, X, Y)\n",
    "            # Log parameter error after each update\n",
    "            if self.true_theta is not None:\n",
    "                theta_error.append(\n",
    "                    np.dot(self.theta - self.true_theta, self.theta - self.true_theta)\n",
    "                )\n",
    "            if self.true_hessian is not None:\n",
    "                hessian_error.append(\n",
    "                    np.linalg.norm(self.hessian - self.true_hessian, ord=\"fro\")\n",
    "                )\n",
    "        if plot:\n",
    "            self.plot_errors(theta_error, hessian_error)\n",
    "        return theta_error, hessian_error\n",
    "\n",
    "    def run_multiple(self, num_runs: int = 10) -> Tuple[List[float], List[float]]:\n",
    "        \"\"\"\n",
    "        Run the experiment multiple times, and return the average error\n",
    "        \"\"\"\n",
    "        self.theta_errors_avg = (\n",
    "            np.zeros(self.true_theta.shape) if self.true_theta is not None else None\n",
    "        )\n",
    "        self.hessian_errors_avg = (\n",
    "            np.zeros(self.true_hessian.shape) if self.true_hessian is not None else None\n",
    "        )\n",
    "        for i in tqdm(range(num_runs), desc=\"Runs\"):\n",
    "            self.generate_initial_theta()\n",
    "            theta_error, hessian_error = self.run()\n",
    "            if self.true_theta is not None:\n",
    "                self.theta_errors_avg += theta_error\n",
    "            if self.true_hessian is not None:\n",
    "                self.hessian_errors_avg += hessian_error\n",
    "        if self.true_theta is not None:\n",
    "            self.theta_errors_avg /= num_runs\n",
    "        if self.true_hessian is not None:\n",
    "            self.hessian_errors_avg /= num_runs\n",
    "        self.plot_errors(self.theta_errors_avg, self.hessian_errors_avg)\n",
    "        return self.theta_errors_avg, self.hessian_errors_avg\n",
    "\n",
    "    def plot_errors(self, theta_error, hessian_error):\n",
    "        \"\"\"\n",
    "        Plot the errors of estimated theta and hessian\n",
    "        \"\"\"\n",
    "        if self.true_theta is not None:\n",
    "            plt.plot(theta_error)\n",
    "            plt.title(\"Parameter error\")\n",
    "            plt.show()\n",
    "        if self.true_hessian is not None:\n",
    "            plt.plot(hessian_error)\n",
    "            plt.title(\"Hessian error (Frobenius norm)\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def create_dataset_logistic(n: int, theta: np.ndarray):\n",
    "    d = len(theta)\n",
    "    X = np.random.randn(n, d - 1)\n",
    "    phi = np.hstack([np.ones((n, 1)), X])\n",
    "    Y = np.random.binomial(1, sigmoid(phi @ theta))\n",
    "    return list(zip(X, Y))  # Maybe use a dataloader for large datasets\n",
    "\n",
    "\n",
    "def g(X: np.ndarray, Y: np.ndarray, h: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute the logistic loss, works only for a batch of data\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    phi = np.hstack([np.ones(n, 1), X])\n",
    "    dot_product = np.dot(phi, h)\n",
    "    return np.log(1 + np.exp(dot_product)) - dot_product * Y\n",
    "\n",
    "\n",
    "def g_grad(X: np.ndarray, Y: np.ndarray, h: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the logistic loss, works only for a single data point\n",
    "    \"\"\"\n",
    "    print(X.shape)\n",
    "    phi = np.hstack([np.ones((1,)), X])\n",
    "    print(phi.shape)\n",
    "    dot_product = np.dot(phi, h)\n",
    "    p = sigmoid(dot_product)\n",
    "    print(p.shape)\n",
    "    print(Y.shape)\n",
    "    # grad = (p - Y)[:, np.newaxis] * phi\n",
    "    grad = (p - Y) * phi  # Equivalent\n",
    "    return grad\n",
    "\n",
    "\n",
    "def g_grad_and_hessian(X, Y, h):\n",
    "    \"\"\"\n",
    "    Compute the gradient and the Hessian of the logistic loss\n",
    "    Does not work for a batch of data because of the outer product\n",
    "    \"\"\"\n",
    "    # For batch data, should work\n",
    "    # n, d = X.shape\n",
    "    # phi = np.hstack([np.ones(n, 1), X])\n",
    "    # dot_product = np.dot(phi, h)\n",
    "    # p = sigmoid(dot_product)\n",
    "    # grad = (p - Y) * phi\n",
    "    # hessian = np.einsum('i,ij,ik->ijk', p * (1 - p), phi, phi)\n",
    "    # return grad, hessian\n",
    "\n",
    "    # For a single data point\n",
    "    phi = np.hstack([np.ones((1,)), X])\n",
    "    dot_product = np.dot(phi, h)\n",
    "    p = sigmoid(dot_product)\n",
    "    grad = (p - Y) * phi\n",
    "    hessian = p * (1 - p) * np.outer(phi, phi)\n",
    "    return grad, hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test broadcast numpy *\n",
    "a = np.array([[1], [2]])  # like (p - Y) for a batch of 2 samples\n",
    "X = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
    "# print(a * x)\n",
    "\n",
    "# test outer product\n",
    "# print(np.outer(x, x))\n",
    "\n",
    "# test np.atleast_2d\n",
    "X = np.atleast_2d(np.array([1, 2, 3]))\n",
    "# print(x.shape)\n",
    "\n",
    "# test np.dot\n",
    "X = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "Y = np.array([[1], [2], [3]])\n",
    "# print(np.dot(x, y))\n",
    "\n",
    "# test np.einsum\n",
    "p = np.array([0.5, 0.6])\n",
    "X = np.array([[1, 2], [3, 4]])\n",
    "print(np.einsum(\"i,ij,ik->ijk\", p * (1 - p), X, X))\n",
    "print(p[0] * (1 - p[0]) * np.outer(X[0], X[0]))\n",
    "print(p[1] * (1 - p[1]) * np.outer(X[1], X[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent optimizer\n",
    "    Uses a learning rate lr = c_mu * iteration^(-mu)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mu: float, c_mu: float):\n",
    "        \"\"\"\n",
    "        Initialize the optimizer\n",
    "        \"\"\"\n",
    "        self.mu = mu\n",
    "        self.c_mu = c_mu\n",
    "        self.iteration = 0\n",
    "\n",
    "    def reset_lr(self):\n",
    "        \"\"\"\n",
    "        Reset the learning rate\n",
    "        \"\"\"\n",
    "        self.iteration = 0\n",
    "\n",
    "    def step(self, X: np.ndarray, Y: np.ndarray, theta: np.ndarray):\n",
    "        \"\"\"\n",
    "        Perform one optimization step\n",
    "        \"\"\"\n",
    "        if self.g_grad is None:\n",
    "            raise ValueError(\"g_grad is not set\")\n",
    "        self.iteration += 1\n",
    "        grad = self.g_grad(X, Y, theta)\n",
    "        lr = self.c_mu * self.iteration ** (-self.mu)\n",
    "        theta = theta - lr * grad\n",
    "        return theta, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "N = 100\n",
    "n = 10_000\n",
    "true_theta = np.array([0, 3, -9, 4, -9, 15, 0, -7, 1, 0])\n",
    "exp = Experiment(g, g_grad, g_grad_and_hessian, true_theta=true_theta)\n",
    "exp.set_e(1)\n",
    "exp.generate_initial_theta()\n",
    "optimizer = SGD(0.5, 0.1)\n",
    "exp.set_optimizer(optimizer)\n",
    "dataset = create_dataset_logistic(n, true_theta)\n",
    "exp.set_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment\n",
    "exp.run(plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(N):\n",
    "#     dataset = create_dataset_logistic(n, true_theta)\n",
    "\n",
    "#     exp = Experiment(\n",
    "#         g, g_grad, g_grad_and_hessian, dataset, optimizer, true_theta=true_theta\n",
    "#     )\n",
    "#     exp.set_theta(np.random.randn(10))\n",
    "#     exp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import functional as autograd_f\n",
    "\n",
    "\n",
    "class NewtonOptim(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1):\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = closure()\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                def func(input):\n",
    "                    return closure()\n",
    "\n",
    "                hessian = autograd_f.hessian(func, p)\n",
    "\n",
    "                hessian_inv = torch.inverse(hessian + 1e-5 * torch.eye(hessian.size(0)))\n",
    "\n",
    "                p.data.sub_(group[\"lr\"] * hessian_inv @ p.grad.data.flatten()).view_as(\n",
    "                    p\n",
    "                )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "theta_true = torch.tensor([1.5, -2.0, 1.0, 0.5, 3.0])\n",
    "X = torch.randn(10000, 5)\n",
    "noise = 0.5 * torch.randn(10000)\n",
    "Y = X @ theta_true + noise\n",
    "dataset = TensorDataset(X, Y)\n",
    "g = nn.Linear(5, 1, bias=False)\n",
    "criterion = nn.MSELoss()\n",
    "experiment = Experiment(\n",
    "    g, dataset, NewtonOptim, lr=0.001, theta_true=theta_true, criterion=criterion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closure():\n",
    "    y_pred = g(X)\n",
    "    loss = criterion(y_pred, Y.view(-1, 1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, Y in experiment.dataloader:\n",
    "    experiment.optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.plot_param_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
