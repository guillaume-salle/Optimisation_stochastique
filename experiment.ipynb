{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import Callable, List, Tuple\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    \"\"\"\n",
    "    Base class for optimizers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self, theta_dim: int) -> None:\n",
    "        \"\"\"\n",
    "        Reset the optimizer state.\n",
    "        Should be implemented by subclasses.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        Y: np.ndarray,\n",
    "        g_grad: Callable,\n",
    "        g_grad_and_hessian: Callable,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Perform one optimization step.\n",
    "        Should be implemented by subclasses.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    \"\"\"\n",
    "    Experiment class to run optimization experiments with second order methods,\n",
    "    on a given function g, with computable gradient and hessian.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        g: Callable,\n",
    "        g_grad: Callable,\n",
    "        g_grad_and_hessian: Callable,\n",
    "        optimizer_list: List[Optimizer],\n",
    "        e: float,\n",
    "        dataset: List[Tuple[np.ndarray, np.ndarray]] = None,\n",
    "        generate_dataseet: Callable = None,\n",
    "        true_theta: np.ndarray = None,\n",
    "        true_hessian_inv: np.ndarray = None,\n",
    "        theta_dim: int = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the experiment\n",
    "        \"\"\"\n",
    "        if dataset is None and generate_dataseet is None:\n",
    "            raise ValueError(\"dataset or create_dataset should be set\")\n",
    "        if true_theta is None and theta_dim is None:\n",
    "            raise ValueError(\"dim_theta is not set\")\n",
    "\n",
    "        self.true_theta = true_theta\n",
    "        self.true_hessian_inv = true_hessian_inv\n",
    "        self.g = g\n",
    "        self.g_grad = g_grad\n",
    "        self.g_grad_and_hessian = g_grad_and_hessian\n",
    "        self.optimizer_list = optimizer_list\n",
    "        self.dataset = dataset\n",
    "        self.generate_dataset = generate_dataseet\n",
    "        self.e = e\n",
    "        self.true_theta = true_theta\n",
    "        self.theta_dim = theta_dim if true_theta is None else true_theta.shape[0]\n",
    "        self.true_hessian_inv = true_hessian_inv\n",
    "\n",
    "    def generate_initial_theta(self):\n",
    "        \"\"\"\n",
    "        Generate a random initial theta\n",
    "        \"\"\"\n",
    "        if self.e is None:\n",
    "            raise ValueError(\"e is not set for generating random theta\")\n",
    "        loc = (\n",
    "            self.true_theta if self.true_theta is not None else np.zeros(self.theta_dim)\n",
    "        )\n",
    "        self.initial_theta = loc + self.e * np.random.randn(self.theta_dim)\n",
    "\n",
    "    def log_estimation_error(self, theta_errors, hessian_inv_errors, optimizer):\n",
    "        if theta_errors is not None:\n",
    "            theta_errors[optimizer.name].append(\n",
    "                np.dot(self.theta - self.true_theta, self.theta - self.true_theta)\n",
    "            )\n",
    "        if hessian_inv_errors is not None and optimizer.hessian_inv is not None:\n",
    "            hessian_inv_errors[optimizer.name].append(\n",
    "                np.linalg.norm(optimizer.hessian_inv - self.true_hessian_inv, ord=\"fro\")\n",
    "            )\n",
    "\n",
    "    def run(self, plot: bool = False) -> Tuple[List[float], List[float]]:\n",
    "        \"\"\"\n",
    "        Run the experiment for a given initial theta, a dataset and a list of optimizers\n",
    "        \"\"\"\n",
    "        if self.initial_theta is None:\n",
    "            raise ValueError(\"initial theta is not set\")\n",
    "        if self.dataset is None:\n",
    "            raise ValueError(\"dataset is not set\")\n",
    "\n",
    "        # Initialize the directories for errors if true values are provided\n",
    "        theta_errors = (\n",
    "            {optimizer.name: [] for optimizer in self.optimizer_list}\n",
    "            if self.true_theta is not None\n",
    "            else None\n",
    "        )\n",
    "        hessian_inv_errors = (\n",
    "            {optimizer.name: [] for optimizer in self.optimizer_list}\n",
    "            if self.true_hessian_inv is not None\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        # Run the experiment for each optimizer\n",
    "        for optimizer in tqdm(self.optimizer_list, desc=\"Optimizers\", leave=False):\n",
    "            self.theta = self.initial_theta.copy()\n",
    "            optimizer.reset(self.theta_dim)\n",
    "            # Log initial error\n",
    "            self.log_estimation_error(theta_errors, hessian_inv_errors, optimizer)\n",
    "\n",
    "            # Online pass on the dataset\n",
    "            for X, Y in tqdm(self.dataset, desc=\"Data\", leave=False):\n",
    "                optimizer.step(X, Y, self.theta, self.g_grad, self.g_grad_and_hessian)\n",
    "                self.log_estimation_error(theta_errors, hessian_inv_errors, optimizer)\n",
    "            # Convert errors to numpy arrays\n",
    "            if theta_errors is not None:\n",
    "                theta_errors[optimizer.name] = np.array(theta_errors[optimizer.name])\n",
    "            if hessian_inv_errors is not None:\n",
    "                hessian_inv_errors[optimizer.name] = np.array(\n",
    "                    hessian_inv_errors[optimizer.name]\n",
    "                )\n",
    "        if plot:\n",
    "            self.plot_errors(theta_errors, hessian_inv_errors)\n",
    "\n",
    "        return theta_errors, hessian_inv_errors\n",
    "\n",
    "    def run_multiple(\n",
    "        self, num_runs: int = 100, n: int = 10_000\n",
    "    ) -> Tuple[List[float], List[float]]:\n",
    "        \"\"\"\n",
    "        Run the experiment multiple times by generating a new dataset and initial theta each time\n",
    "        \"\"\"\n",
    "        if self.true_theta is None or self.generate_dataset is None:\n",
    "            raise ValueError(\"true_theta and/or create_dataset are not set\")\n",
    "\n",
    "        # length of error arrays is n + 1 for initial error\n",
    "        self.theta_errors_avg = (\n",
    "            {optimizer.name: np.zeros(n + 1) for optimizer in self.optimizer_list}\n",
    "            if self.true_theta is not None\n",
    "            else None\n",
    "        )\n",
    "        self.hessian_inv_errors_avg = (\n",
    "            {optimizer.name: np.zeros(n + 1) for optimizer in self.optimizer_list}\n",
    "            if self.true_hessian_inv is not None\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        for i in tqdm(range(num_runs), desc=\"Runs\"):\n",
    "            self.dataset = self.generate_dataset(n, self.true_theta)\n",
    "            self.generate_initial_theta()\n",
    "            theta_errors, hessian_inv_errors = self.run()\n",
    "            if self.true_theta is not None:\n",
    "                for name, errors in theta_errors.items():\n",
    "                    self.theta_errors_avg[name] += errors\n",
    "            if self.true_hessian_inv is not None:\n",
    "                for name, errors in hessian_inv_errors.items():\n",
    "                    self.hessian_inv_errors_avg[name] += errors\n",
    "        if self.true_theta is not None:\n",
    "            for name, errors in self.theta_errors_avg.items():\n",
    "                errors /= num_runs\n",
    "        if self.true_hessian_inv is not None:\n",
    "            for name, errors in self.hessian_inv_errors_avg.items():\n",
    "                errors /= num_runs\n",
    "        self.plot_errors(self.theta_errors_avg, self.hessian_inv_errors_avg)\n",
    "        return self.theta_errors_avg, self.hessian_inv_errors_avg\n",
    "\n",
    "    def plot_errors(self, theta_errors, hessian_inv_errors):\n",
    "        \"\"\"\n",
    "        Plot the errors of estimated theta and hessian inverse of all optimizers\n",
    "        \"\"\"\n",
    "        if self.true_theta is not None:\n",
    "            for name, errors in theta_errors.items():\n",
    "                plt.plot(errors, label=name)\n",
    "            plt.xlabel(\"n\")\n",
    "            plt.ylabel(\"theta estimation squared error\")\n",
    "            plt.title(f\"e = {self.e}\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        if self.true_hessian_inv is not None:\n",
    "            for name, errors in hessian_inv_errors.items():\n",
    "                plt.plot(errors, label=name)\n",
    "            plt.xlabel(\"n\")\n",
    "            plt.ylabel(\"hessian inverse estimation error\")\n",
    "            plt.title(f\"e = {self.e}\")\n",
    "            plt.legend()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def create_dataset_logistic(n: int, true_theta: np.ndarray):\n",
    "    d = len(true_theta)\n",
    "    X = np.random.randn(n, d - 1)\n",
    "    phi = np.hstack([np.ones((n, 1)), X])\n",
    "    Y = np.random.binomial(1, sigmoid(phi @ true_theta))\n",
    "    return list(zip(X, Y))  # Maybe use a dataloader for large datasets\n",
    "\n",
    "\n",
    "def g(X: np.ndarray, Y: np.ndarray, h: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute the logistic loss, works only for a batch of data\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    phi = np.hstack([np.ones(n, 1), X])\n",
    "    dot_product = np.dot(phi, h)\n",
    "    return np.log(1 + np.exp(dot_product)) - dot_product * Y\n",
    "\n",
    "\n",
    "def g_grad(X: np.ndarray, Y: np.ndarray, h: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the logistic loss, works only for a single data point\n",
    "    \"\"\"\n",
    "    phi = np.hstack([np.ones((1,)), X])\n",
    "    dot_product = np.dot(phi, h)\n",
    "    p = sigmoid(dot_product)\n",
    "    # grad = (p - Y)[:, np.newaxis] * phi\n",
    "    grad = (p - Y) * phi  # Equivalent\n",
    "    return grad\n",
    "\n",
    "\n",
    "def g_grad_and_hessian(X, Y, h):\n",
    "    \"\"\"\n",
    "    Compute the gradient and the Hessian of the logistic loss\n",
    "    Does not work for a batch of data because of the outer product\n",
    "    \"\"\"\n",
    "    # For batch data, should work\n",
    "    # n, d = X.shape\n",
    "    # phi = np.hstack([np.ones(n, 1), X])\n",
    "    # dot_product = np.dot(phi, h)\n",
    "    # p = sigmoid(dot_product)\n",
    "    # grad = (p - Y) * phi\n",
    "    # hessian = np.einsum('i,ij,ik->ijk', p * (1 - p), phi, phi)\n",
    "    # return grad, hessian\n",
    "\n",
    "    # For a single data point\n",
    "    phi = np.hstack([np.ones((1,)), X])\n",
    "    dot_product = np.dot(phi, h)\n",
    "    p = sigmoid(dot_product)\n",
    "    grad = (p - Y) * phi\n",
    "    hessian = p * (1 - p) * np.outer(phi, phi)\n",
    "    return grad, hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test broadcast numpy *\n",
    "a = np.array([[1], [2]])  # like (p - Y) for a batch of 2 samples\n",
    "X = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
    "# print(a * x)\n",
    "\n",
    "# test outer product\n",
    "# print(np.outer(x, x))\n",
    "\n",
    "# test np.atleast_2d\n",
    "X = np.atleast_2d(np.array([1, 2, 3]))\n",
    "# print(x.shape)\n",
    "\n",
    "# test np.dot\n",
    "X = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "Y = np.array([[1], [2], [3]])\n",
    "# print(np.dot(x, y))\n",
    "\n",
    "# test np.einsum\n",
    "p = np.array([0.5, 0.6])\n",
    "X = np.array([[1, 2], [3, 4]])\n",
    "print(np.einsum(\"i,ij,ik->ijk\", p * (1 - p), X, X))\n",
    "print(p[0] * (1 - p[0]) * np.outer(X[0], X[0]))\n",
    "print(p[1] * (1 - p[1]) * np.outer(X[1], X[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent optimizer\n",
    "    Uses a learning rate lr = c_mu * iteration^(-mu)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mu: float, c_mu: float):\n",
    "        self.name = \"SGD\"\n",
    "        self.mu = mu\n",
    "        self.c_mu = c_mu\n",
    "        self.iteration = 0\n",
    "\n",
    "    def reset(self, theta_dim: int):\n",
    "        \"\"\"\n",
    "        Reset the optimizer state\n",
    "        \"\"\"\n",
    "        self.iteration = 0\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        Y: np.ndarray,\n",
    "        theta: np.ndarray,\n",
    "        g_grad: Callable,\n",
    "        g_grad_and_hessian: Callable,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Perform one optimization step\n",
    "        \"\"\"\n",
    "        self.iteration += 1\n",
    "        grad = g_grad(X, Y, theta)\n",
    "        learning_rate = self.c_mu * self.iteration ** (-self.mu)\n",
    "        theta += -learning_rate * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNA(Optimizer):\n",
    "    \"\"\"\n",
    "    Stochastic Newton Algorithm optimizer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mu: float, c_mu: float):\n",
    "        self.name = \"SNA\"\n",
    "        self.mu = mu\n",
    "        self.c_mu = c_mu\n",
    "        self.iteration = 0\n",
    "\n",
    "    def reset(self, theta_dim: int):\n",
    "        \"\"\"\n",
    "        Reset the learning rate and estimate of the hessian\n",
    "        \"\"\"\n",
    "        self.iteration = 0\n",
    "        self.hessian = np.eye(theta_dim)\n",
    "        self.hessian_inv = np.eye(theta_dim)\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        Y: np.ndarray,\n",
    "        theta: np.ndarray,\n",
    "        g_grad: Callable,\n",
    "        g_grad_and_hessian: Callable,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Perform one optimization step\n",
    "        \"\"\"\n",
    "        self.iteration += 1\n",
    "        grad, hessian = g_grad_and_hessian(X, Y, theta)\n",
    "        self.hessian += (hessian - self.hessian) / (self.iteration + 1)\n",
    "        try:\n",
    "            self.hessian_inv = np.linalg.inv(self.hessian)\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(\"Hessian is not invertible\")\n",
    "        learning_rate = self.c_mu * self.iteration ** (-self.mu)\n",
    "        theta += -learning_rate * self.hessian_inv @ grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "N = 10\n",
    "n = 10_0000\n",
    "true_theta = np.array([0, 3, -9, 4, -9, 15, 0, -7, 1, 0])\n",
    "optimizer_list = [SGD(0.66, 1), SNA(1, 1)]\n",
    "e = 2\n",
    "\n",
    "exp = Experiment(\n",
    "    g,\n",
    "    g_grad,\n",
    "    g_grad_and_hessian,\n",
    "    optimizer_list,\n",
    "    e,\n",
    "    true_theta=true_theta,\n",
    "    generate_dataseet=create_dataset_logistic,\n",
    ")\n",
    "exp.generate_initial_theta()\n",
    "exp.dataset = exp.generate_dataset(n, true_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment\n",
    "theta_errors, _ = exp.run(plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_error_avg, _ = exp.run_multiple(num_runs=N)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
