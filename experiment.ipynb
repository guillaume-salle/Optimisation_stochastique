{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    \"\"\"\n",
    "    Experiment class to run optimization experiments with second order methods\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        g,\n",
    "        g_grad,\n",
    "        g_grad_and_hessian,\n",
    "        dataset,\n",
    "        optimizer,\n",
    "        true_theta=None,\n",
    "        true_hessian=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the experiment\n",
    "        \"\"\"\n",
    "        self.true_theta = true_theta\n",
    "        self.true_hessian = true_hessian\n",
    "        self.g = g\n",
    "        self.g_grad = g_grad\n",
    "        self.g_grad_and_hessian = g_grad_and_hessian\n",
    "        self.optimizer = optimizer\n",
    "        self.dataset = dataset\n",
    "        if true_theta is not None:\n",
    "            self.true_theta = true_theta\n",
    "            self.theta_error = [np.dot(theta - true_theta, theta - true_theta)]\n",
    "        if true_hessian is not None:\n",
    "            self.true_hessian = true_hessian\n",
    "            self.hessian_error = [\n",
    "                np.linalg.norm(self.hessian - true_hessian, ord=\"fro\")\n",
    "            ]\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Run the experiment\n",
    "        \"\"\"\n",
    "        if self.theta is None:\n",
    "            raise ValueError(\"Initial theta not set\")\n",
    "        self.hessian = np.eye(self.theta.shape)\n",
    "        self.optimizer.reset_lr()\n",
    "        for x, y in tqdm(self.dataset):\n",
    "            self.theta, self.hessian = self.optimizer.step(\n",
    "                self.theta,\n",
    "                self.hessian,\n",
    "                x,\n",
    "                y,\n",
    "                self.g,\n",
    "                self.g_grad,\n",
    "                self.g_grad_and_hessian,\n",
    "            )\n",
    "            # Log parameter error after each update\n",
    "            if self.true_theta is not None:\n",
    "                self.theta_error.append(\n",
    "                    np.dot(self.theta - self.true_theta, self.theta - self.true_theta)\n",
    "                )\n",
    "            if self.true_hessian is not None:\n",
    "                self.hessian_error.append(\n",
    "                    np.linalg.norm(self.hessian - self.true_hessian, ord=\"fro\")\n",
    "                )\n",
    "        self.plot_errors()\n",
    "        self.theta = None  # Reset theta\n",
    "\n",
    "    def plot_errors(self):\n",
    "        \"\"\"\n",
    "        Plot the errors\n",
    "        \"\"\"\n",
    "        if self.true_theta is not None:\n",
    "            plt.plot(self.theta_error)\n",
    "            plt.title(\"Parameter error\")\n",
    "            plt.show()\n",
    "        if self.true_hessian is not None:\n",
    "            plt.plot(self.hessian_error)\n",
    "            plt.title(\"Hessian error (Frobenius norm)\")\n",
    "            plt.show()\n",
    "\n",
    "    def set_theta(self, theta):\n",
    "        \"\"\"\n",
    "        Set the initial theta\n",
    "        \"\"\"\n",
    "        self.theta = theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def create_dataset_logistic(n, theta):\n",
    "    d = len(theta)\n",
    "    X = np.random.randn(n, d - 1)\n",
    "    phi = np.hstack([np.ones((n, 1)), X])\n",
    "    Y = np.random.binomial(1, sigmoid(phi @ theta))\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "X, Y = create_dataset_logistic(n, true_theta)\n",
    "dataset = zip(X, Y)\n",
    "\n",
    "\n",
    "def g(h, X, Y):\n",
    "    n, d = X.shape\n",
    "    phi = np.hstack([np.ones(n, 1), X])\n",
    "    dot_product = np.dot(phi, h)\n",
    "    return np.log(1 + np.exp(dot_product)) - dot_product * Y\n",
    "\n",
    "\n",
    "def g_grad(h, X, Y):\n",
    "    n, d = X.shape\n",
    "    phi = np.hstack([np.ones(n, 1), X])\n",
    "    dot_product = np.dot(phi, h)\n",
    "    p = sigmoid(dot_product)\n",
    "    # grad = (p - Y)[:, np.newaxis] * X\n",
    "    grad = (p - Y) * X  # Equivalent\n",
    "    return grad\n",
    "\n",
    "\n",
    "def g_grad_and_hessian(h, X, Y):\n",
    "    \"\"\"\n",
    "    Compute the gradient and the Hessian of the logistic loss\n",
    "    Does not work for a batch of data because of the outer product\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    phi = np.hstack([np.ones(n, 1), X])\n",
    "    dot_product = np.dot(phi, h)\n",
    "    p = sigmoid(dot_product)\n",
    "    grad = (p - Y) * X\n",
    "    hessian = np.einsum(\"i,ij,ik->ijk\", p * (1 - p), X, X)\n",
    "    return grad, hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test broadcast numpy *\n",
    "a = np.array([[1], [2]])  # like (p - Y) for a batch of 2 samples\n",
    "x = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
    "# print(a * x)\n",
    "\n",
    "# test outer product\n",
    "# print(np.outer(x, x))\n",
    "\n",
    "# test np.atleast_2d\n",
    "x = np.atleast_2d(np.array([1, 2, 3]))\n",
    "# print(x.shape)\n",
    "\n",
    "# test np.dot\n",
    "x = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "y = np.array([[1], [2], [3]])\n",
    "# print(np.dot(x, y))\n",
    "\n",
    "# test np.einsum\n",
    "p = np.array([0.5, 0.6])\n",
    "x = np.array([[1, 2], [3, 4]])\n",
    "print(np.einsum(\"i,ij,ik->ijk\", p * (1 - p), x, x))\n",
    "print(p[0] * (1 - p[0]) * np.outer(x[0], x[0]))\n",
    "print(p[1] * (1 - p[1]) * np.outer(x[1], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "N = 100\n",
    "n = 10_000\n",
    "true_theta = np.array([0, 3, -9, 4, -9, 15, 0, -7, 1, 0])\n",
    "for i in range(N):\n",
    "    X, Y = create_dataset_logistic(n, true_theta)\n",
    "    dataset = zip(X, Y)\n",
    "    exp = Experiment(\n",
    "        g, g_grad, g_grad_and_hessian, dataset, optimizer, true_theta=true_theta\n",
    "    )\n",
    "\n",
    "    exp.set_theta(np.random.randn(10))\n",
    "    exp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import functional as autograd_f\n",
    "\n",
    "\n",
    "class NewtonOptim(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1):\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = closure()\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                def func(input):\n",
    "                    return closure()\n",
    "\n",
    "                hessian = autograd_f.hessian(func, p)\n",
    "\n",
    "                hessian_inv = torch.inverse(hessian + 1e-5 * torch.eye(hessian.size(0)))\n",
    "\n",
    "                p.data.sub_(group[\"lr\"] * hessian_inv @ p.grad.data.flatten()).view_as(\n",
    "                    p\n",
    "                )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "theta_true = torch.tensor([1.5, -2.0, 1.0, 0.5, 3.0])\n",
    "X = torch.randn(10000, 5)\n",
    "noise = 0.5 * torch.randn(10000)\n",
    "y = X @ theta_true + noise\n",
    "dataset = TensorDataset(X, y)\n",
    "g = nn.Linear(5, 1, bias=False)\n",
    "criterion = nn.MSELoss()\n",
    "experiment = Experiment(\n",
    "    g, dataset, NewtonOptim, lr=0.001, theta_true=theta_true, criterion=criterion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closure():\n",
    "    y_pred = g(X)\n",
    "    loss = criterion(y_pred, y.view(-1, 1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in experiment.dataloader:\n",
    "    experiment.optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.plot_param_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
